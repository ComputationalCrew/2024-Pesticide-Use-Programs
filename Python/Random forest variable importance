### Random forest variable importance

# linear algebra
import numpy as np 
# data processing
import pandas as pd 
# data visualization
import seaborn as sns
%matplotlib inline
from matplotlib import pyplot as plt
from matplotlib import style
# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from collections import Counter
from IPython.core.display import display, HTML

# Data
data = pd.read_csv('.../Clean_Cluster_Random Forest_V1.csv')
dataset.info()

# Original
features1 = ['Northwest_quadrant',	'Northeast_quadrant',	'Southwest_quadrant',	'Southeast_quadrant',	
             'Susceptibility_to_V6_strains','Susceptibility_to_non-V6_strains',
             'Seasonal_disease_incidence',	'Flag_shoot_incidence', 'Pruning_thoroughness',	
             'Initial_strain',	 'Disease_incidence_April', 'Disease_incidence_May',	
             'Disease_incidence_June',	 'Disease_incidence_July',	
             'Degree_centrality_June_R6',	'Degree_centrality_July_R6',	'Degree_centrality_June_non-R6',	
             'Degree_centrality_July_non-R6',	'Program_group']

# Selected
features1 = ['Northwest_quadrant',	'Southwest_quadrant',	'Southeast_quadrant',	
             'Susceptibility_to_V6_strains','Susceptibility_to_non-V6_strains',
             'Seasonal_disease_incidence',	'Flag_shoot_incidence', 'Pruning_thoroughness',	
             'Initial_strain',	 'Disease_incidence_April', 'Disease_incidence_May',	
             'Disease_incidence_June',	 'Disease_incidence_July',	
             'Degree_centrality_July_R6', 'Degree_centrality_July_non-R6',	'Program_group']

X = dataset[features1]
y = dataset['AnnualCost'] 
y = dataset['AnnualIngredients']

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
print("Shape of X_train: ",X_train.shape)
print("Shape of X_test: ", X_test.shape)
print("Shape of y_train: ",y_train.shape)
print("Shape of y_test",y_test.shape)

# 0. Best parameter
n_estimators = [5,20,50,100] # number of trees in the random forest
max_features = ['auto', 'sqrt'] # number of features in consideration at every split
max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree
min_samples_split = [2, 6, 10] # minimum sample number to split a node
min_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node
bootstrap = [True, False] # method used to sample data points
random_grid = {'n_estimators': n_estimators,
'max_features': max_features,
'max_depth': max_depth,
'min_samples_split': min_samples_split,
'min_samples_leaf': min_samples_leaf,
'bootstrap': bootstrap}
# Importing Random Forest Regressor from the sklearn.ensemble
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
from sklearn.model_selection import RandomizedSearchCV
rf_random = RandomizedSearchCV(estimator = rf,param_distributions = random_grid,
               n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(X_train, y_train)
print ('Random grid: ', random_grid, '\n')
# print the best parameters
print ('Best Parameters: ', rf_random.best_params_, ' \n')
# Annual Cost
rf = RandomForestRegressor(n_estimators = 100, min_samples_split = 6, min_samples_leaf= 1, max_features = 'sqrt', max_depth= 20, bootstrap=True) 
rf.fit(X_train, y_train) 
# Annual Ingredient
rf = RandomForestRegressor(n_estimators = 50, min_samples_split = 10, min_samples_leaf= 1, max_features = 'sqrt', max_depth= 50, bootstrap=False) 
rf.fit(X_train, y_train) 
# Use the forest's predict method on the test data
predictions = rf.predict(X)

### Table 4: variable Importance 

# Determine feature importance values
import numpy as np 
importances = rf.feature_importances_
# Obtaining feature names
rf.feature_names_in_
# Visualize the feature importance
import matplotlib.pyplot as plt 
plt.figure(figsize=(6, 6),  dpi = 1000)
plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]), importances[sorted_indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)
plt.tight_layout()
plt.show()

# Figure 7: variable importance computed with SHAP values
pip install shap
import shap
from matplotlib import pyplot as plt
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, plot_type="bar")
plt.figure(figsize=(15, 15),  dpi = 1000)
shap.summary_plot(shap_values, X_test)

# Table 4: explore optimal number of features effect on performance
from sklearn.feature_selection import RFECV
rfecv = RFECV(rf,step=1, min_features_to_select=1, cv=None, scoring=None, verbose=0, n_jobs=None, importance_getter='auto')
rfecv.fit(X,y)
rfecv.support_
rfecv.ranking_ 
rfecv.estimator_.feature_importances_ 
features = rfecv.get_feature_names_out()
# Make predictions using the selected features 
y_pred = rfecv.predict(X)
# Get the optimal number of features selected
optimal_num_features = rfecv.n_features_
# Get the mean test scores during the feature selection process
mean_test_scores = rfecv.cv_results_['mean_test_score']
# Print the results
print("Optimal number of features selected:", optimal_num_features)
print("Number of steps in RFECV:", len(mean_test_scores))
from operator import itemgetter
features = X_train.columns.to_list()
for x, y in (sorted(zip(rfecv.ranking_ , features), key=itemgetter(0))):
    print(x, y)
